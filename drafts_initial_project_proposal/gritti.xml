<?xml version="1.0" encoding="UTF-8"?>
<indexing>
 <object index="9" name="model-response-message-contentr_aac7a6a73d49b6c6" object_type="section"/>
 <paragraph index="10" node_type="writer" parent_index="9">Subject: Final Project Proposal: Comparative Study of Vision-Based Robotic Sorting</paragraph>
 <paragraph index="11" node_type="writer" parent_index="9">Dear Professor,</paragraph>
 <paragraph index="12" node_type="writer" parent_index="9">For my final project, I plan to implement and compare two different methods for an autonomous sorting task.</paragraph>
 <paragraph index="13" node_type="writer" parent_index="9">Project Objective: A rtb.models.Panda() arm, observed by a simulated camera, will autonomously sort &quot;berries&quot; (colored spheres) that appear at random locations on a tabletop. The robot must use its camera feed to locate the berries and place them into their corresponding color-coded bins.</paragraph>
 <paragraph index="14" node_type="writer" parent_index="9">Approach (The Core Comparison): I will solve this problem in two distinct ways to compare a classical robotics approach against a modern machine learning one.</paragraph>
 <paragraph index="15" node_type="writer" parent_index="9">Method 1: Classical CV &amp; Motion Planning</paragraph>
 <paragraph index="16" node_type="writer" parent_index="9">Sensing: I will use OpenCV to process the camera image, using HSV color masking to find the pixel coordinates of each berry.</paragraph>
 <paragraph index="17" node_type="writer" parent_index="9">Control: After translating pixel data to world coordinates, I will use standard roboticstoolbox functions (like ikine_...) to plan and execute explicit pick-and-place trajectories.</paragraph>
 <paragraph index="18" node_type="writer" parent_index="9">Method 2: End-to-End Reinforcement Learning</paragraph>
 <paragraph index="19" node_type="writer" parent_index="9">Sensing &amp; Control: I will use a Convolutional Neural Network (CnnPolicy) that takes the raw camera image as direct input.</paragraph>
 <paragraph index="20" node_type="writer" parent_index="9">Training: The agent will be trained (using stable-baselines3) in a headless swift simulation to learn its own policy for both &quot;seeing&quot; the berries and &quot;acting&quot; to sort them, based on a reward function.</paragraph>
 <paragraph index="21" node_type="writer" parent_index="9">Tools: This project will leverage the full suite of libraries we've used: roboticstoolbox, spatialmath, spatialgeometry, and swift, supplemented by OpenCV and stable-baselines3.</paragraph>
</indexing>
